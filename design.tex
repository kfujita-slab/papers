\chapter{設計と実装}
本章では、当プロジェクトにおいてソフトウェア実装されているシステムの
ネットワーク構造について説明し、
そのハードウェア実装に際しての設計・実装方法について述べる。
\section{アルゴリズム}
\label{sec:algorithm}
本システムは、入力された手術画像に対して、ニューラルネットワーク
を用いたセグメンテーションを行い、ラベル画像を出力する。
入力はサイズ$640\times512$のRGB画像であり、出力として4種類の
クラスラベルを返す。クラスは、背景・胆嚢・胆嚢管・総胆管とする。
%これを基に、設計したシステムの構成図は、図\ref{segment_design}のようになる。
%\begin{figure}[hbt]
%    \centering
%    \includegraphics[scale=1.0,clip]{image/segment_design.pdf}
%    \caption{システムの構成}
%    \label{fig:segment_design}
%\end{figure}
%以下にネットワークの説明とともに各操作の概要を述べる。
\subsection{ネットワーク構成}
\label{sec:net_design}
本プロジェクトで実装されているネットワークの全体図を図\ref{fig:segment_soft}に示す。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=0.94\textwidth,clip]{image/segment_soft.pdf}
    \caption{ネットワーク全体図}
    \label{fig:segment_soft}
\end{figure}

図\ref{fig:segment_soft}に則して説明を行う。まず入力画像はExtという小規模ネットワークに
与えられ、特徴マップが生成される。Extは入力がRGBの3チャネル、出力が$n$チャネルと
なっており、$n$はパラメータで任意に設定できる。続いて、特徴マップは$2\times2$ max poolingで縮小された後、Rdcという小規模ネットワークに渡される。Rdcは入出力ともに$n$チャネル
である。poolingによる縮小とRdcネットワークの適用を再帰的に繰り返すことで、
低次元への特徴マップの集約を行い、大域の情報を利用できるようにする。
最下層に到達した後、unpoolingによる拡張が
行われ、縮小処理を経ていない特徴マップと共にItgという小規模ネットワークに渡される。
よってItgの入力は$2n$チャネル、出力は$n$チャネルである。
unpoolingによる拡張と前段の特徴マップの統合によって、
pooling層で失われた位置情報の復元を狙う。
この拡張とItgによる統合は、縮小とRdcの適用と同回数行われる。
最後に、元画像と同じ大きさとなった特徴マップに$1\times1$畳み込み演算を行い、
指定の4クラスに対する尤度マップを生成する。

以下の項目では、ここに挙げた各処理について詳細を述べる。

\subsection{畳み込み層}
\label{sec:conv}
本ネットワークの畳み込み層は3つの小規模ネットワークから構成されている。
図\ref{fig:three_Net}にそれぞれの形状を示す。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=8cm,clip]{image/three_Net.pdf}
    \caption{3種の小規模ネットワーク}
    \label{fig:three_Net}
\end{figure}

フィルタサイズはいずれも$3\times3$であり、入力と出力のサイズを同じにするため
各層でパディングが行われている。
活性化関数には、式\ref{LeakyReLU}に示すLeaky ReLU\cite{Leaky_ReLU}を用いる。
Leaky ReLUを用いる理由だが、式\ref{ReLU}に示したReLUは
いかなる負の入力に対しても0を出力するため、学習が進むにつれて
絶対値の大きい負数が出力されることがある。
本研究ではハードウェア化にあたり固定小数点演算を採用していることから、
この現象は演算に必要なビット幅の増大や演算精度の低下につながる。
一方Leaky ReLUは、負領域においても0でない重み$a$を持つため、この問題を緩和できる。
$a$の値については、ビットシフト演算による
単純な実装が可能な$a=0.25=2^{-2}$を採用した。
\begin{equation}
    \label{LeakyReLU}
    f(x) = \max(ax,x) ,\ a = 0.25 \quad (\mathrm{Leaky\ ReLU})
\end{equation}

本ネットワークとU-Netの差異として、
小規模ネットワークの再帰的な適用が挙げられる。
医用画像はデータセットが乏しいため、
訓練データを丸暗記してしまう過学習を引き起こす可能性が高い。
そこで、小規模ネットワークの組み合わせによってネットワークを構築することで、
記憶できるパラメータ数を意図的に減らし過学習を抑制する。
加えて、異なる解像度のデータに対するネットワークの再帰的な適用により、
サイズに左右されない、より普遍的な特徴抽出が期待できる点も、過学習抑制に繋がる。

また、各層でパディングが行われるのもU-Netとの差異として挙げられる。
これは、入力と出力のサイズを変化させないことで、設計を単純化できるためである。
%方が、設計上好ましいだろうという判断に基づいている。特に、
パディングを行わない場合、
図\ref{fig:padding}に示すようにpooling・unpoolingにおける動作が
複雑化することが予測できる。
各層でのパディングにより、
小規模ネットワーク適用後、
poolingでは入力を半分に縮小し、unpoolingでは入力を2倍に拡張する
という単純な動作となるため、設計を単純化できるという利点がある。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth,clip]{image/padding.pdf}
    \caption{パディングの有無によるサイズ変化の違い}
    \label{fig:padding}
\end{figure}

\subsection{pooling層}
\label{sec:pooling}
本システムのpooling層では、一般的なCNNと同様に
$2\times2$ max poolingを適用する。
入力は$2\times2$の領域に区切られ、各領域内の最大値1画素が出力される。
よって、pooling層の出力は前段からの入力に対し、縦横ともに半分のサイズとなる。
図\ref{fig:pooling}にその様子を示す。\ref{sec:conv}節で述べたように、pooling層の前後で実行される
畳み込み層においてサイズは変化しないため、pooling層でのサイズ
変更は単純かつ規則的であることが確認できる。
\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth,clip]{image/pooling.pdf}
    \caption{pooling層の適用による縮小}
    \label{fig:pooling}
\end{figure}
\subsection{unpooling層}
\label{sec:unpooling}
本システムのunpooling層で行われている処理は
図\ref{fig:unpooling}とは異なり、
値を周辺画素に転写する処理となっているため、
位置情報を記憶する必要はない。pooling層を経ていない特徴マップとサイズを一致させるには、
\ref{sec:conv}節で述べたように
縦横のサイズを2倍にすればよい。
また、\ref{sec:net_design}節で述べたように、unpooling層の適用はpooling層の適用と
同回数行われ、最終的な出力のサイズは最初の入力と一致する。
unpooling層の動作を図\ref{fig:unpooling_design}に示す。
\begin{figure}[hbt]
    \centering
    \includegraphics[width=\textwidth,clip]{image/unpooling_design.pdf}
    \caption{unpooling層の適用による拡大}
    \label{fig:unpooling_design}
\end{figure}

\input{rtl_design}

\section{学習}
\subsection{ツール}
学習には、Python上で動作するニューラルネットワーク向けフレームワーク
であるChainer5.3.0\ \cite{Chainer}を用いる。Chainerでは、複雑なデータ構造を
簡潔な記述で構築でき、CUDAによるGPUを用いた高速な学習も可能である。
また、学習および評価における各処理には、数値計算ライブラリのNumpyや、画像処理
ライブラリのOpenCVを用いる。
\subsection{学習データ}
今回ニューラルネットワークの学習に用いる
画像データと教師ラベルからなるデータセットは183組であり、
このうち138組を訓練用データセット、45組を評価用データセットとして学習を行う。
画像データは長崎大学病院から提供された
実際の手術画像であり、画像サイズは640$\times$512となっている。
また、それらの画像を医師が目視で判断し、手動でラベリングを行い作成された
画像を教師データとした。これらの画像例を以下の図\ref{fig:train_pic}、
図\ref{fig:theacher_label}に示す。

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/train01.pdf}
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/train02.pdf}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/train03.pdf}
    \end{subfigure}
    \caption{実際の手術画像データ}
    \label{fig:train_pic}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/label01.pdf}
    \end{subfigure}
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/label02.pdf}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.3\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{image/label03.pdf}
    \end{subfigure}
    \caption{教師データ}
    \label{fig:theacher_label}
\end{figure}

\subsection{量子化手法}
ソフトウェアでの実装においては、各演算は32ビットの浮動小数点型で行われ、
学習の結果得られる重みやバイアス等も同じ型となっている。
しかしながら、ハードウェア実装においてパラメータに浮動小数点型を
用いるのは資源容量的に厳しく、固定小数点型を利用するのが望ましい。

そのため、本来ならば学習においても固定小数点型を利用して学習を進め、
パラメータを決定付けるべきだが、Chainerでは浮動小数点型を用いることが
前提となっており、固定小数点型で動作させるのは困難である。
そこで、学習は浮動小数点型で行い、得られたパラメータを固定小数点型に変換して
実装することとした。

%関連論文\cite{sample}.
%
%図の参照、図\ref{fig:yoshiki}
%
%表の参照、表\ref{tab:sample}
%
%\begin{figure}[hbt]
%    \centering
%    \includegraphics[scale=1.0,clip]{image/yoshiki_nico.jpg}
%    \caption{Sample図}
%    \label{fig:yoshiki}
%\end{figure}
%
%
%\begin{table}[hbt]
%    \caption{Sample表}
%    \centering
%    \begin{tabular}{|c||c|c|c|c|c|}
%        \hline
%        & XXX & XX & XXXXX & XXXX & XXXX\\
%        \hline\hline
%        XXXX     & xxx & xxx & xxxx & xx & xx \\\hline
%        XXXXX & xxx & xxx & xxxx & xx & xx \\\hline   
%    \end{tabular}
%    \label{tab:sample}
%\end{table}






