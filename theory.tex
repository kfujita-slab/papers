\chapter{理論}
本章では、システムの実装に用いる理論についての説明を行う。
\section{ニューラルネットワーク}
ニューラルネットワーク(Neural Network)は、生物の脳内におけるニューロン(神経細胞)の結びつき方をモデルにした情報処理システムである。
学習能力を持つため、サンプルとなるデータに基づき必要とされる機能を自動形成することができる。
\begin{figure}[hbt]
    \centering
    \includegraphics[width=7cm,clip]{image/neuron_image.pdf}
    \caption{各ニューロンにおけるプロセスの模式図$(n=2)$}
    \label{fig:neuron_image}
\end{figure}

ニューラルネットワークを構成するニューロンの模式図を、前段のニューロン数$n$が2の
ときを例として
図\ref{fig:neuron_image}に示す。ニューロン$i$の出力$y_i$は、入力$x_j(1\leq j\leq n)$を基に、\ref{conv_math}式により決定される。
\begin{equation}
    \label{conv_math}
    y_i = h(b_i + \sum_{j=1}^{n}x_jw_{i,j} )
\end{equation}

各入力$x_j$には固有の値である重み$w_{i,j}$が設定され、同じく固有の値であるバイアス$b_i$と共に計算に用いられる。
この固有の値こそがニューラルネットワークの機能を決定づける要素であり、適切な値に設定することで必要とする機能を形成する。
また、$h$は活性化関数と呼ばれる関数であり、以下のような非線形関数が用いられることが多い。
%\begin{equation}
%    \label{sigmoid}
%    h(x) = (1 + \mathrm{e}^{-x})^{-1}\quad (標準シグモイド関数)
%\end{equation}
%\begin{equation}
%    \label{ReLU}
%    h(x) = \max(0,x)\quad(\mathrm{ReLU})
%\end{equation}
\begin{align}
    \label{sigmoid}
    h(x) &= (1 + \mathrm{e}^{-x})^{-1}\quad (標準シグモイド関数) \\
    \label{ReLU}
    h(x) &= \max(0,x)\quad\quad(\mathrm{ReLU})
\end{align}

\begin{figure}[hbt]
    \centering
    \includegraphics[width=7cm,clip]{image/NN_image.pdf}
    \caption{ニューラルネットワークの例}
    \label{fig:NN_image}
\end{figure}

ニューラルネットワークの代表的な構造として、順伝播型ニューラルネットワークの1種である多層パーセプトロンを例に挙げる。
中間層が1層以上存在する多層パーセプトロンでは、任意の連続関数を近似可能であることが知られている。
%ただし、活性化関数に線形関数を用いた場合、どんなに層を増やしたとしてもそれと等価な単層ネットワークが存在するため、
ただし、線形な関数のみで構成されるネットワークは、
どんなに層を増やしたとしても
それと等価な単層ネットワークが存在するため、
層を増やす恩恵を得るには非線形な関数が各層で用いられる必要がある。
そのため、一般的に活性化関数には非線形関数が用いられている。

先述した通り、ニューラルネットワークで期待される機能を実現するには、各ニューロンの重みとバイアスを適切に設定しなければならない。この調整をデータに基づいて自動で行う仕組みが学習である。
学習には大きく分けて教師あり学習と教師なし学習があるが、ここでは教師あり学習について説明を行う。

教師あり学習では、入力データと教師データが対になって与えられたデータセットを利用し、ネットワークに入力データを渡した際の出力が教師データと
一致するように重みやバイアスを変化させていく。
一般的に、出力精度の指標としては2乗和誤差や交差エントロピー誤差などの損失関数が、学習手法としては誤差逆伝播法が用いられる。
入力データに基づく出力を$y_k$、それと対応する教師データを$t_k$として以下に
損失関数の例を示す。
\begin{itemize}
\item 2乗和誤差 $\displaystyle E = \frac{1}{2} \sum_{k} {(y_k - t_k)}^2$
\item 交差エントロピー誤差 $\displaystyle E = - \sum_k t_k \log y_k$
\end{itemize}

損失関数とは、出力精度の悪さを示す指標であり、
これによって得られた誤差を最小とするように重みおよびバイアスを変更する。
また、重みに基づいて誤差を逆伝播させ、中間層の重みおよびバイアスを順次更新していく。
損失関数は逆伝播可能な関数ではなくてはならず、活性化関数との
組み合わせによっては学習が遅くなることもある。
そのため、深層学習においては上記の損失関数が用いられることが多い。
%損失関数を用いる理由だが、例えば物体認識を行うニューラルネットワークの精度評価を正答率とした場合、
%パラメータの更新に対する正答率の変化は離散的な値しか観測できず、パラメータの更新はある地点で停止することになる。
%このように、微小な変更に対して正しい精度評価を得るために損失関数が用いられている。

\section{畳み込みニューラルネットワーク}
畳み込みニューラルネットワーク(Convolutional Neural Network：CNN)は、
図\ref{fig:NN_image}に示したような、隣接する層の全ニューロン間で
結合がある(全結合)ニューラルネットワークとは異なり、入力と設定されたフィルタの畳み込み演算に基づいて出力の決定を行うニューラルネットワークの一種である。
CNNは画像認識や音声認識など活用形態は多岐にわたるが、現在のディープラーニングを用いた画像認識における手法の多くがCNNをベースとしており、
本研究においても画像認識を目的としてCNNを利用しているため、
2次元画像にCNNを適用する場合について説明を行う。

画像認識における全結合ネットワークの問題は、空間的情報が失われる点にある。
入力が2次元画像の場合、空間的に近いピクセルは似たような値である、距離の離れたピクセルは互いに影響を及ぼさない、画素のRGB値には密接な関連があるなど、空間的形状には汲み取るべき本質的なパターンが含まれているはずである。
しかし全結合層はこのような形状を無視し、すべて同等のニューロンとして処理を行う
ため、空間的情報を生かすことができない。

そこでCNNは形状を維持するために畳み込み層を利用する。畳み込み層で行う処理は画像処理におけるフィルタ演算である。具体的な処理を図\ref{fig:convolution}に示す。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=13cm,clip]{image/convolution.pdf}
    \caption{畳み込み層：畳み込み演算を「＊」で表記}
    \label{fig:convolution}
\end{figure}

\noindent なお、$n\times n$サイズの入力画像$X(i,j)$と、
$m\times m$サイズのフィルタ$F(i,j)$の畳み込み演算の定義は式\ref{eq:conv}の
ように表せる。
\begin{equation}
\label{eq:conv}
(X \ast F)(i,j) = \sum^{m-1}_{k=0} \sum^{m-1}_{l=0}X(i+k,j+l)F(k,l)
\end{equation}
%
%式もほしいーーーー
%\begin{equation}
%    
%\end{equation}
%

以上のように、フィルタと入力データの対応する要素の積和演算を行い、対応する場所へ格納していく。
このフィルタこそが重みに対応するパラメータであり、フィルタの値がCNNの動作を決定付ける。バイアスは全結合ニューラルネットワークと同様に、
フィルタ(重み)の適用後に加算される。

図\ref{fig:convolution}でもみられたように、フィルタの適用により出力(特徴マップ)は
入力データに比べて一回り小さくなる。これを回避するために、入力データの端に0を
追加する(ゼロパディング)操作を加える場合がある。
これにより、図\ref{fig:convolution}のように、フィルタの大きさが3$\times$3かつ、
フィルタの適用範囲を動かす量(ストライド)が1ならば、
入力と出力の大きさを一致させることができる。
フィルタやストライドの大きさが変化する場合は、それを考慮してパディングの
大きさも変化させる必要がある。

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm,clip]{image/maxpooling.pdf}
    \caption{pooling (max pooling)}
    \label{fig:maxpooling}
\end{figure}

CNNを利用したアプリケーションでは、多くの場合位置不変性(パターンの位置がずれていても影響を受け辛い性質)が求められるため、プーリング層と呼ばれる新たな層がネットワークに追加される。画像を一定領域に区切り、各領域を最大値や平均値を用いて1画素に置き換える処理(pooling)により、パターンのずれを吸収させ、位置不変性を高める。
なお、当然ながらプーリング層において学習するパラメータはなく、層間における
画素ごとのニューロン数(チャネル)の変化もない。ただし、縦横方向の空間は小さくなるため、画素ごとの位置情報は失われる。
元々の位置情報が必要となるアプリケーションなどでは注意が必要となる。

\section{セマンティックセグメンテーション}
一般的なCNNによる物体認識は、画像全体に対して何らかの判定・分類を行うものが多い。
例としては、写真群から猫が写っている画像だけを抽出する機能などが挙げられる。
一方で、セマンティックセグメンテーションは画素ごとに判定・分類を行う。
上記の例に合わせるなら、複数の物体が写っている写真に対して、猫が写っている部分だけ塗りつぶしを行う機能が挙げられる。
セグメンテーションの例を図\ref{fig:segment_cat}に示す。
猫とソファが区別されて塗り分けられている様子が確認できる。

\begin{figure}[hbt]
    \centering
    \includegraphics[width=10cm,clip]{image/segment_cat.pdf}
    \caption{セグメンテーションの例、\cite{segment_example}より引用}
    \label{fig:segment_cat}
\end{figure}

当然、1画素だけを見てその画素が何にあたるか判定するのは人間でも不可能であるため、周辺画素ひいては画像全体を見て判定する必要がある。
%そこで画素の空間的情報を用いられるCNNの出番となるのだが、ここで一つ問題が発生する。
%CNNではプーリング層の適用によって画素の位置情報が失われてしまうことだ。
そこで、先述のCNNを用いて空間的情報を利用した判定を行うのだが、
CNNにはプーリング層の適用によって画素の位置情報が破棄されるという問題が存在する。

医用画像セグメンテーションを目的としたネットワークであるU-Net\cite{U-Net}では、
poolingによって縮小された特徴マップを
後段で拡張することによって位置情報の復元を行う。
この操作はpoolingに対してunpooling(アンプーリング)と呼称される。
\begin{figure}[hbt]
    \centering
    \includegraphics[width=10cm,clip]{image/unpooling.pdf}
    \caption{unpooling}
    \label{fig:unpooling}
\end{figure}
U-Netにおけるunpooling処理を図\ref{fig:unpooling}に示す。
poolingの際に最大値だった画素の位置を保持しておき、
unpoolingでは保持された位置を利用して拡張を行う。
また、U-Netはプーリング層に通す前の特徴マップを後段に直接連結させることで、
粗大化を避けられないアンプーリング層の出力を補佐し、
セグメンテーションの精度を高めている。
%なお、本研究で実装するセグメンテーションシステムは、U-Netを基盤としたネットワークによって構築されている。
\section{FPGA}
Field Programmable Gate Array(FPGA)は、製造後に設計者が構成を設定できる
集積回路である。
FPGAは、複数入力のルックアップテーブル(LUT)等で構成された論理ブロックを
多数搭載し、LUTを書き換えることによって論理積や論理和といった様々な論理を
表現できる。
また、論理ブロック間を結ぶ内部配線についても構成を変化させることができるため、
設計者は論理を表現したブロックを適切に組み合わせることによって任意の論理回路を実装する。

FPGAの設計は、一般的にVelilog HDLやVHDLといったハードウェア記述言語で行われる。
用途に合わせて設計される集積回路である
ASIC (Application Specific Integrated Circuit)に比べ、
集積密度や電力効率、動作速度では劣る一方、
開発・製造期間は短く、設計の変更も容易である。
また、コスト面に関しても、製造のための初期コストは不要であり、
FPGA自体は汎用品であることから、少量生産においては生産コストでも有利である。

一般的に、ソフトウェアで動画像処理をするときは、
動作クロック周波数の高い高性能CPUが必要となる。
一方ハードウェアは、回路を並列化やパイプライン化することで
処理性能を挙げることができ、ソフトウェアと比較して低い動作クロックで
同等の処理を実現できる。そのため、画像処理システムにはFPGAを始めとする
ハードウェアが用いられることが多い。
また、計算分野によっては処理の並列化特性から高性能なCPU・GPUよりも高速に動作
する可能性がある\cite{tsunami}。CNNもチャネル方向の並列化に加え、
画素情報を順次走査で受け取りながらのストリーム処理が可能な点から
ハードウェアに適した計算処理であるといえる。

%\subsection{サブセクション}
%関連論文\cite{sample}.
%
%図の参照、図\ref{fig:yoshiki}
%
%表の参照、表\ref{tab:sample}
%
%\begin{figure}[hbt]
%    \centering
%    \includegraphics[scale=1.0,clip]{image/yoshiki_nico.jpg}
%    \caption{Sample図}
%    \label{fig:yoshiki}
%\end{figure}
%
%
%\begin{table}[hbt]
%    \caption{Sample表}
%    \centering
%    \begin{tabular}{|c||c|c|c|c|c|}
%        \hline
%        & XXX & XX & XXXXX & XXXX & XXXX\\
%        \hline\hline
%        XXXX     & xxx & xxx & xxxx & xx & xx \\\hline
%        XXXXX & xxx & xxx & xxxx & xx & xx \\\hline   
%    \end{tabular}
%    \label{tab:sample}
%\end{table}






